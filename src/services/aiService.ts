/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *   TITANEâˆ v16.1 â€” AI SERVICE ENGINE (OFFLINE FIRST)
 *   Mode: Local > Cloud (APIs on-demand only)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

import { getAIConfig, isOnlineModeEnabled } from '../config/offline-first';
import { confirmCloudAPIUsage } from '../utils/cloudAPIConfirmation';

const GEMINI_API_KEY = import.meta.env.VITE_GEMINI_API_KEY || '';
const GEMINI_API_URL = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent';
const OLLAMA_API_URL = 'http://localhost:11434/api/generate';

export interface AIMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: number;
}

export interface AIResponse {
  content: string;
  provider: 'gemini' | 'ollama' | 'fallback';
  timestamp: number;
}

/**
 * Nettoie et valide un message utilisateur
 */
function sanitizeMessage(message: string): string {
  return message
    .trim()
    .replace(/<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi, '')
    .substring(0, 10000); // Max 10k caractÃ¨res
}

/**
 * Appelle l'API Gemini
 */
async function callGemini(message: string, history: AIMessage[] = []): Promise<AIResponse> {
  if (!GEMINI_API_KEY) {
    throw new Error('Gemini API key not configured');
  }

  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), 30000); // 30s timeout

  try {
    const response = await fetch(`${GEMINI_API_URL}?key=${GEMINI_API_KEY}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [
          {
            parts: [
              {
                text: `Tu es TITANEâˆ, une IA avancÃ©e intÃ©grÃ©e dans un systÃ¨me d'auto-Ã©volution cognitive. Contexte: ${history.slice(-3).map(m => `${m.role}: ${m.content}`).join('\n')}\n\nUtilisateur: ${message}`
              }
            ]
          }
        ],
        generationConfig: {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 2048,
        }
      }),
      signal: controller.signal,
    });

    clearTimeout(timeout);

    if (!response.ok) {
      throw new Error(`Gemini API error: ${response.status}`);
    }

    const data = await response.json();
    const content: string = data.candidates?.[0]?.content?.parts?.[0]?.text || 'RÃ©ponse vide';

    return {
      content,
      provider: 'gemini',
      timestamp: Date.now(),
    };
  } catch (error) {
    clearTimeout(timeout);
    console.error('Gemini API error:', error);
    throw error;
  }
}

/**
 * Appelle Ollama en local
 */
async function callOllama(message: string, history: AIMessage[] = []): Promise<AIResponse> {
  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), 30000);

  try {
    const contextPrompt = history.length > 0
      ? `Contexte prÃ©cÃ©dent:\n${history.slice(-3).map(m => `${m.role}: ${m.content}`).join('\n')}\n\n`
      : '';

    const response = await fetch(OLLAMA_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'llama2',
        prompt: `${contextPrompt}Tu es TITANEâˆ, une IA cognitive avancÃ©e. RÃ©ponds en franÃ§ais.\n\nUtilisateur: ${message}\n\nTITANEâˆ:`,
        stream: false,
      }),
      signal: controller.signal,
    });

    clearTimeout(timeout);

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status}`);
    }

    const data = await response.json();
    const content: string = data.response || 'RÃ©ponse vide';

    return {
      content,
      provider: 'ollama',
      timestamp: Date.now(),
    };
  } catch (error) {
    clearTimeout(timeout);
    console.error('Ollama API error:', error);
    throw error;
  }
}

/**
 * RÃ©ponse de fallback quand tout Ã©choue
 */
function getFallbackResponse(): AIResponse {
  const responses: string[] = [
    "Je suis TITANEâˆ, mais mes services IA sont temporairement indisponibles. VÃ©rifie ta connexion et tes clÃ©s API.",
    "SystÃ¨mes IA en mode dÃ©gradÃ©. Impossible de gÃ©nÃ©rer une rÃ©ponse pour le moment.",
    "Erreur de connexion aux services IA. Assure-toi que Gemini API ou Ollama sont configurÃ©s.",
  ];

  return {
    content: responses[Math.floor(Math.random() * responses.length)] as string,
    provider: 'fallback',
    timestamp: Date.now(),
  };
}

/**
 * Point d'entrÃ©e principal : OFFLINE FIRST
 * Local (Ollama) > Cloud (Gemini si activÃ©) > Fallback
 */
export async function askTitan(
  message: string,
  history: AIMessage[] = []
): Promise<AIResponse> {
  const sanitized = sanitizeMessage(message);

  if (!sanitized) {
    throw new Error('Message vide ou invalide');
  }

  const config = getAIConfig();
  const onlineEnabled = isOnlineModeEnabled();

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // PRIORITÃ‰ 1 : LOCAL FIRST (Ollama)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  if (config.localFirst || !onlineEnabled) {
    try {
      console.log('ğŸ¤– [LOCAL FIRST] Tentative Ollama...');
      const response = await callOllama(sanitized, history);
      console.log('âœ… Ollama OK');
      return response;
    } catch (error) {
      console.warn('âš ï¸ Ollama non disponible');
      
      // Si mode local strict, pas de fallback cloud
      if (!onlineEnabled) {
        console.log('ğŸ  Mode Local strict - Fallback local');
        return getFallbackResponse();
      }
    }
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // PRIORITÃ‰ 2 : CLOUD (Gemini) - SI ACTIVÃ‰ UNIQUEMENT
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  if (onlineEnabled && GEMINI_API_KEY) {
    // Demander confirmation utilisateur
    const confirmed = await confirmCloudAPIUsage(
      'Gemini AI',
      'Ollama local non disponible'
    );
    
    if (confirmed) {
      try {
        console.log('ğŸŒ [CLOUD MODE] Tentative Gemini API...');
        const response = await callGemini(sanitized, history);
        console.log('âœ… Gemini OK');
        return response;
      } catch (error) {
        console.warn('âš ï¸ Gemini Ã©chouÃ©');
      }
    } else {
      console.log('âŒ AccÃ¨s cloud refusÃ© par l\'utilisateur');
    }
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // FALLBACK : RÃ©ponses locales
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  console.log('âš ï¸ Fallback local activÃ©');
  return getFallbackResponse();
}

/**
 * Stream une rÃ©ponse (si supportÃ© par le provider)
 */
export async function* streamTitanResponse(
  message: string,
  history: AIMessage[] = []
): AsyncGenerator<string> {
  // Pour l'instant, on simule le streaming avec la rÃ©ponse complÃ¨te
  // TODO: ImplÃ©menter vrai streaming Gemini/Ollama
  const response = await askTitan(message, history);
  
  // Simule le streaming caractÃ¨re par caractÃ¨re
  for (let i = 0; i < response.content.length; i++) {
    yield response.content[i] as string;
    await new Promise(resolve => setTimeout(resolve, 20)); // 20ms delay
  }
}

export default {
  askTitan,
  streamTitanResponse,
};
